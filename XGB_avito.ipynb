{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "#from matplotlib_venn import venn2, venn2_circles\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = pd.read_csv('aggregated_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train = train.merge(gp, on='user_id', how='left')\n",
    "test = test.merge(gp, on='user_id', how='left')\n",
    "\n",
    "agg_cols = list(gp.columns)[1:]\n",
    "\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_region_unique = pd.read_csv(\"avito_region_city_features.csv\")\n",
    "city_region_unique.drop('city_region', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(city_region_unique, how=\"left\", on=[\"region\", \"city\"])\n",
    "test = test.merge(city_region_unique, how=\"left\", on=[\"region\", \"city\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['description'].fillna('unknowndescription', inplace=True)\n",
    "    df['title'].fillna('unknowntitle', inplace=True)\n",
    "\n",
    "    df['weekday'] = pd.to_datetime(df['activation_date']).dt.day\n",
    "    \n",
    "    for col in ['description', 'title']:\n",
    "        df['num_words_' + col] = df[col].apply(lambda comment: len(comment.split()))\n",
    "        df['num_unique_words_' + col] = df[col].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "\n",
    "    df['words_vs_unique_title'] = df['num_unique_words_title'] / df['num_words_title'] * 100\n",
    "    df['words_vs_unique_description'] = df['num_unique_words_description'] / df['num_words_description'] * 100\n",
    "    \n",
    "    df['city'] = df['region'] + '_' + df['city']\n",
    "    df['num_desc_punct'] = df['description'].apply(lambda x: count(x, set(string.punctuation)))\n",
    "    df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "    df[\"price\"].fillna(-999,inplace=True)\n",
    "    df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "    df[\"category_parent_name\"] = df[\"category_name\"] + ' ' + df['parent_category_name']\n",
    "    for col in agg_cols:\n",
    "        df[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_title = CountVectorizer(stop_words=stopwords.words('russian'), lowercase=True, min_df=25)\n",
    "\n",
    "title_counts = count_vectorizer_title.fit_transform(train['title'].append(test['title']))\n",
    "\n",
    "train_title_counts = title_counts[:len(train)]\n",
    "test_title_counts = title_counts[len(train):]\n",
    "\n",
    "\n",
    "count_vectorizer_desc = TfidfVectorizer(stop_words=stopwords.words('russian'), \n",
    "                                        lowercase=True, ngram_range=(1, 2),\n",
    "                                        max_features=15000)\n",
    "\n",
    "desc_counts = count_vectorizer_desc.fit_transform(train['description'].append(test['description']))\n",
    "\n",
    "train_desc_counts = desc_counts[:len(train)]\n",
    "test_desc_counts = desc_counts[len(train):]\n",
    "\n",
    "train_title_counts.shape, train_desc_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_eng_title = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True, min_df=25)\n",
    "\n",
    "title_counts = count_vectorizer_eng_title.fit_transform(train_eng['en_title'].append(test_eng['en_title']))\n",
    "\n",
    "train_eng_title_counts = title_counts[:len(train_eng)]\n",
    "test_eng_title_counts = title_counts[len(train_eng):]\n",
    "\n",
    "\n",
    "count_vectorizer_eng_desc = TfidfVectorizer(stop_words=stopwords.words('english'), \n",
    "                                        lowercase=True, ngram_range=(1, 2),\n",
    "                                        max_features=15000)\n",
    "\n",
    "desc_counts = count_vectorizer_eng_desc.fit_transform(train_eng['en_desc'].append(test_eng['en_desc']))\n",
    "\n",
    "train_eng_desc_counts = desc_counts[:len(train_eng)]\n",
    "test_eng_desc_counts = desc_counts[len(train_eng):]\n",
    "\n",
    "train_eng_title_counts.shape, train_eng_desc_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'deal_probability'\n",
    "predictors = [\n",
    "    'num_desc_punct', \n",
    "    'words_vs_unique_description', 'num_unique_words_description', 'num_unique_words_title', 'num_words_description', 'num_words_title',\n",
    "    'avg_times_up_user', 'avg_days_up_user', 'n_user_items', \n",
    "    'price', 'item_seq_number','num'\n",
    "]\n",
    "categorical = [\n",
    "    'image_top_1', 'param_1', 'param_2', 'param_3', \n",
    "    'city', 'region', 'category_name', 'parent_category_name', 'user_type'\n",
    "]\n",
    "\n",
    "predictors = predictors + categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_eng, test_eng\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical_idx(col, df_trn, df_test, drop_uniques=0):\n",
    "    merged = pd.concat([df_trn[col], df_test[col]])\n",
    "    if drop_uniques != 0:\n",
    "        unique, inverse, counts = np.unique(merged, return_counts=True, return_inverse=True)\n",
    "        unique_with_zeros = np.select([counts < drop_uniques, counts >= drop_uniques], [unique * 0, unique])\n",
    "        merged = unique_with_zeros[inverse]\n",
    "\n",
    "    train_size = df_trn[col].shape[0]\n",
    "    idxs, uniques = pd.factorize(merged)\n",
    "    \n",
    "    return idxs[:train_size], idxs[train_size:], uniques\n",
    "tr_userid, te_userid, tknzr_userid = to_categorical_idx('user_id', train, test, drop_uniques=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['trans_user_id'] = tr_userid\n",
    "test['trans_user_id'] =te_userid\n",
    "predictors.append('trans_user_id')\n",
    "del tr_userid, te_userid, tknzr_userid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_feats = ['latitude', 'longitude',\n",
    "       'lat_lon_hdbscan_cluster_05_03', 'lat_lon_hdbscan_cluster_10_03',\n",
    "       'lat_lon_hdbscan_cluster_20_03', 'region_id', 'city_region_id']\n",
    "for cf in c_feats:\n",
    "    predictors.append(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_feats =['lat_lon_hdbscan_cluster_05_03', 'lat_lon_hdbscan_cluster_10_03',\n",
    "       'lat_lon_hdbscan_cluster_20_03']\n",
    "for ccf in cc_feats:\n",
    "    categorical.append(ccf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = pd.read_csv('region_income.csv')\n",
    "train = train.merge(income, on='region', how='left')\n",
    "test = test.merge(income, on='region', how='left')\n",
    "predictors.append('income')\n",
    "city_pop = pd.read_csv('city_population.csv')\n",
    "gp = city_pop.groupby(['city'])[['population']]\n",
    "gp_df = pd.DataFrame()\n",
    "gp_df['population'] = gp.sum()['population']\n",
    "gp_df.reset_index(inplace=True)\n",
    "gp_df.rename(index=str, columns={'index': 'city'})\n",
    "train = train.merge(gp_df, on='city', how='left')\n",
    "test = test.merge(gp_df, on='city', how='left')\n",
    "predictors.append('population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_map = {\"Свердловская область\" : \"Sverdlovsk oblast\",\n",
    "            \"Самарская область\" : \"Samara oblast\",\n",
    "            \"Ростовская область\" : \"Rostov oblast\",\n",
    "            \"Татарстан\" : \"Tatarstan\",\n",
    "            \"Волгоградская область\" : \"Volgograd oblast\",\n",
    "            \"Нижегородская область\" : \"Nizhny Novgorod oblast\",\n",
    "            \"Пермский край\" : \"Perm Krai\",\n",
    "            \"Оренбургская область\" : \"Orenburg oblast\",\n",
    "            \"Ханты-Мансийский АО\" : \"Khanty-Mansi Autonomous Okrug\",\n",
    "            \"Тюменская область\" : \"Tyumen oblast\",\n",
    "            \"Башкортостан\" : \"Bashkortostan\",\n",
    "            \"Краснодарский край\" : \"Krasnodar Krai\",\n",
    "            \"Новосибирская область\" : \"Novosibirsk oblast\",\n",
    "            \"Омская область\" : \"Omsk oblast\",\n",
    "            \"Белгородская область\" : \"Belgorod oblast\",\n",
    "            \"Челябинская область\" : \"Chelyabinsk oblast\",\n",
    "            \"Воронежская область\" : \"Voronezh oblast\",\n",
    "            \"Кемеровская область\" : \"Kemerovo oblast\",\n",
    "            \"Саратовская область\" : \"Saratov oblast\",\n",
    "            \"Владимирская область\" : \"Vladimir oblast\",\n",
    "            \"Калининградская область\" : \"Kaliningrad oblast\",\n",
    "            \"Красноярский край\" : \"Krasnoyarsk Krai\",\n",
    "            \"Ярославская область\" : \"Yaroslavl oblast\",\n",
    "            \"Удмуртия\" : \"Udmurtia\",\n",
    "            \"Алтайский край\" : \"Altai Krai\",\n",
    "            \"Иркутская область\" : \"Irkutsk oblast\",\n",
    "            \"Ставропольский край\" : \"Stavropol Krai\",\n",
    "            \"Тульская область\" : \"Tula oblast\"}\n",
    "regional = pd.read_csv(\"regional.csv\", index_col = [0])\n",
    "train['region_en'] = train['region'].apply(lambda x : region_map[x])\n",
    "test['region_en'] = test['region'].apply(lambda x : region_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rDense = regional[\"Density_of_region(km2)\"]\n",
    "rRural = regional[\"Rural_%\"]\n",
    "rTime_zone = regional[\"Time_zone\"]\n",
    "rPopulation = regional[\"Total_population\"]\n",
    "rUrban = regional[\"Urban%\"]\n",
    "reg_index = np.array([regional.index[i].lower() for i in range(len(regional))])\n",
    "rDense.index = reg_index\n",
    "rRural.index = reg_index\n",
    "rTime_zone.index = reg_index\n",
    "rPopulation.index = reg_index\n",
    "rUrban.index = reg_index\n",
    "\n",
    "df_region = train[\"region_en\"]\n",
    "\n",
    "reg_dense = np.array([rDense[df_region[i].lower()] for i in range(len(train))])\n",
    "reg_rural = np.array([rRural[df_region[i].lower()] for i in range(len(train))])\n",
    "reg_Time_zone = np.array([rTime_zone[df_region[i].lower()] for i in range(len(train))])\n",
    "reg_Population = np.array([rPopulation[df_region[i].lower()] for i in range(len(train))])\n",
    "reg_Urban = np.array([rUrban[df_region[i].lower()] for i in range(len(train))])\n",
    "\n",
    "train[\"reg_dense\"] = reg_dense\n",
    "train[\"rural\"] = reg_rural\n",
    "train[\"reg_Time_zone\"] = reg_Time_zone\n",
    "train[\"reg_Population\"] = reg_Population\n",
    "train[\"reg_Urban\"] = reg_Urban\n",
    "\n",
    "reg_dense = np.array([rDense[df_region[i].lower()] for i in range(len(test))])\n",
    "reg_rural = np.array([rRural[df_region[i].lower()] for i in range(len(test))])\n",
    "reg_Time_zone = np.array([rTime_zone[df_region[i].lower()] for i in range(len(test))])\n",
    "reg_Population = np.array([rPopulation[df_region[i].lower()] for i in range(len(test))])\n",
    "reg_Urban = np.array([rUrban[df_region[i].lower()] for i in range(len(test))])\n",
    "\n",
    "test[\"reg_dense\"] = reg_dense\n",
    "test[\"rural\"] = reg_rural\n",
    "test[\"reg_Time_zone\"] = reg_Time_zone\n",
    "test[\"reg_Population\"] = reg_Population\n",
    "test[\"reg_Urban\"] = reg_Urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reg_dense,reg_rural,reg_Time_zone,reg_Population,reg_Urban,rDense,rRural,rTime_zone\n",
    "del rPopulation,rUrban,reg_index,regional\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_feats = ['reg_dense','rural','reg_Time_zone','reg_Population','reg_Urban']\n",
    "for demf in demo_feats:\n",
    "    predictors.append(demf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical.append('reg_Time_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['reg_Urban'] = train['reg_Urban'].apply(lambda x: x/100)\n",
    "test['reg_Urban'] = test['reg_Urban'].apply(lambda x: x/100)\n",
    "train['rural'] = train['rural'].apply(lambda x: x/100)\n",
    "test['rural'] = test['rural'].apply(lambda x: x/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_feat_col = ['category_name','parent_category_name', 'city','region', 'user_type','param_1','param_2']\n",
    "for gfc in get_feat_col:\n",
    "    use_colm = ['mean','max','std','min']\n",
    "    use_colm.append(gfc)\n",
    "    gp = pd.read_csv(gfc + \"_per_day_stats.csv\", usecols=use_colm)\n",
    "    gp.rename(columns={\"mean\":gfc+\"_mean\",\n",
    "                       \"std\" :gfc+\"_std\",\n",
    "                       \"max\" :gfc+\"_max\",\n",
    "                       \"min\" :gfc+\"_min\"},inplace=True)\n",
    "    train = train.merge(gp, on=gfc, how='left')\n",
    "    test = test.merge(gp, on=gfc, how='left')\n",
    "    use_colm.remove(gfc)\n",
    "    for uc in use_colm:\n",
    "        predictors.append(gfc+'_'+uc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = pd.read_csv('train.csv')\n",
    "train_active = pd.read_csv('train_active.csv')\n",
    "test_tmp = pd.read_csv('test.csv')\n",
    "test_active = pd.read_csv('test_active.csv')\n",
    "all_samples = pd.concat([\n",
    "    train_tmp,\n",
    "    train_active,\n",
    "    test_tmp,\n",
    "    test_active\n",
    "]).reset_index(drop=True)\n",
    "all_samples.drop_duplicates(['item_id'], inplace=True)\n",
    "del train_active, train_tmp\n",
    "del test_active, test_tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = all_samples.merge(city_region_unique, how=\"left\", on=[\"region\", \"city\"])\n",
    "del city_region_unique\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'image_top_1', 'param_1', 'param_2', 'param_3', \n",
    "    'city', 'region', 'category_name', 'parent_category_name', 'user_type'\n",
    "]\n",
    "for cate in categoricals:\n",
    "    all_samples[cate] = all_samples[cate].fillna('unknown')\n",
    "    train[cate] = train[cate].fillna('unknown')\n",
    "    test[cate] = test[cate].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples['filtered_price'] = all_samples['price'].apply(lambda x: 0 if x<0 else x)\n",
    "all_samples['filtered_price'] = all_samples['filtered_price'].apply(lambda x: 999999 if x>=1000000 else x)\n",
    "all_samples['filtered_price'] = all_samples['filtered_price'].apply(lambda x: 0 if x<0 else x)\n",
    "all_samples[\"filtered_price\"] = np.log(all_samples[\"filtered_price\"]+0.001)\n",
    "all_samples['norm_price'] = all_samples['filtered_price'] / all_samples.groupby('param_2')['filtered_price'].transform('sum')\n",
    "all_samples['bin'] = pd.cut(all_samples['norm_price'], np.linspace(0.0,1.0, num=50))\n",
    "all_samples['bin_2'] = pd.cut(all_samples['norm_price'], np.linspace(0.0,1.0, num=1000))\n",
    "all_samples['bin_3'] = pd.cut(all_samples['norm_price'], np.linspace(0.0,1.0, num=10000))\n",
    "gps = all_samples.groupby(['bin_3','param_2'])['item_id'].count().reset_index().rename(columns={'item_id':'price_bin3_count'})\n",
    "all_samples = all_samples.merge(gps, on=['bin_3','param_2'], how='left')\n",
    "gps = all_samples.groupby(['bin_2','param_2'])['item_id'].count().reset_index().rename(columns={'item_id':'price_bin2_count'})\n",
    "all_samples = all_samples.merge(gps, on=['bin_2','param_2'], how='left')\n",
    "gps = all_samples.groupby(['bin','param_2'])['item_id'].count().reset_index().rename(columns={'item_id':'price_bin_count'})\n",
    "all_samples = all_samples.merge(gps, on=['bin','param_2'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('price_bin_count.csv')\n",
    "    \n",
    "train = train.merge(final, on='item_id', how='left')\n",
    "test = test.merge(final, on='item_id', how='left')\n",
    "del final \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['price_temp'] = train['price']\n",
    "test['price_temp'] = test['price']\n",
    "all_samples['price_temp'] = all_samples['price']\n",
    "\n",
    "train['item_seq_number_temp'] = train['item_seq_number']\n",
    "test['item_seq_number_temp'] = test['item_seq_number']\n",
    "all_samples['item_seq_number_temp'] = all_samples['item_seq_number']\n",
    "\n",
    "train['price'] = train['price'].astype('str')\n",
    "test['price'] = test['price'].astype('str')\n",
    "all_samples['price'] = all_samples['price'].astype('str')\n",
    "\n",
    "train['item_seq_number'] = train['item_seq_number'].astype('str')\n",
    "test['item_seq_number'] = test['item_seq_number'].astype('str')\n",
    "all_samples['item_seq_number'] = all_samples['item_seq_number'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['param_1', 'param_2', 'param_3']\n",
    "for ft in feats:\n",
    "    train[ft] = train[ft].astype('str')\n",
    "    test[ft] = test[ft].astype('str')\n",
    "    all_samples[ft] = all_samples[ft].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'image_top_1'\n",
    "]\n",
    "for feature in categoricals:\n",
    "    print(f'Transforming {feature}...')\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(all_samples[feature].astype(str))\n",
    "    \n",
    "    train[feature] = encoder.transform(train[feature].astype(str))\n",
    "    test[feature] = encoder.transform(test[feature].astype(str))\n",
    "train['image_top_1'] = train['image_top_1'].astype('str')\n",
    "test['image_top_1'] = test['image_top_1'].astype('str')\n",
    "all_samples['image_top_1'] = all_samples['image_top_1'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "empty = []\n",
    "naddfeat= 59\n",
    "for i in range(50,naddfeat):\n",
    "    if i==0: selcols=['item_seq_number', 'param_1','param_2','param_3','item_id']; QQ=0;\n",
    "    if i==1: selcols=['item_seq_number', 'param_3', 'category_name', 'parent_category_name','item_id']; QQ=0;\n",
    "    if i==2: selcols=['region', 'city', 'category_name','bin_3','item_id']; QQ=0;\n",
    "    if i==3: selcols=['price', 'param_3','item_id']; QQ=0;\n",
    "    if i==4: selcols=['price','param_1','param_2','param_3', 'item_id']; QQ=0;\n",
    "    if i==5: selcols=['price','category_name', 'parent_category_name', 'item_id']; QQ=0;\n",
    "    if i==6: selcols=['price', 'city', 'region','item_id']; QQ=0;\n",
    "    if i==7: selcols=['price', 'image_top_1','item_id']; QQ=0;\n",
    "    if i==8: selcols=['price', 'bin_3','item_id']; QQ=0;\n",
    "    if i==9: selcols=['price', 'bin_2','bin_3','category_name' ,'item_id']; QQ=0;\n",
    "    if i==10: selcols=['image_top_1','bin_2','bin_3', 'item_id']; QQ=0;\n",
    "    if i==11: selcols=['image_top_1', 'city', 'region','item_id']; QQ=0;\n",
    "    if i==12: selcols=['image_top_1', 'category_name', 'parent_category_name','item_id']; QQ=0;\n",
    "    if i==13: selcols=['image_top_1', 'category_name', 'parent_category_name','user_type','item_id']; QQ=0;\n",
    "    if i==14: selcols=['image_top_1','price','user_type','item_id']; QQ=0;\n",
    "    if i==15: selcols=['item_seq_number', 'param_1','param_2','param_3','item_id']; QQ=0;\n",
    "    if i==16: selcols=['item_seq_number', 'param_3', 'category_name', 'parent_category_name','item_id']; QQ=0;\n",
    "    if i==17: selcols=['region', 'city', 'category_name','bin_3']; QQ=4;\n",
    "    if i==18: selcols=['price', 'param_3']; QQ=4;\n",
    "    if i==19: selcols=['price','param_1','param_2','param_3']; QQ=4;\n",
    "    if i==20: selcols=['price','category_name', 'parent_category_name']; QQ=4;\n",
    "    if i==21: selcols=['price', 'city', 'region']; QQ=4;\n",
    "    if i==22: selcols=['price', 'image_top_1']; QQ=4;\n",
    "    if i==23: selcols=['price', 'bin_3']; QQ=4;\n",
    "    if i==24: selcols=['price', 'bin_2','bin_3','category_name' ]; QQ=4;\n",
    "    if i==25: selcols=['image_top_1','bin_2','bin_3']; QQ=4;\n",
    "    if i==26: selcols=['image_top_1', 'city', 'region']; QQ=4;\n",
    "    if i==27: selcols=['image_top_1', 'category_name', 'parent_category_name']; QQ=4;\n",
    "    if i==28: selcols=['image_top_1', 'category_name', 'parent_category_name','user_type']; QQ=4;\n",
    "    if i==29: selcols=['lat_lon_hdbscan_cluster_05_03','price','category_name']; QQ=4;\n",
    "    if i==30: selcols=['lat_lon_hdbscan_cluster_10_03','price','category_name']; QQ=4;\n",
    "    if i==31: selcols=['lat_lon_hdbscan_cluster_20_03','price','category_name']; QQ=4;\n",
    "    if i==32: selcols=['lat_lon_hdbscan_cluster_05_03','bin_2','category_name','item_id']; QQ=0;\n",
    "    if i==33: selcols=['lat_lon_hdbscan_cluster_10_03','bin_2','category_name','item_id']; QQ=0;\n",
    "    if i==34: selcols=['lat_lon_hdbscan_cluster_20_03','bin_2','category_name','item_id']; QQ=0;\n",
    "    if i==35: selcols=['lat_lon_hdbscan_cluster_05_03','bin','parent_category_name','item_id']; QQ=0;\n",
    "    if i==36: selcols=['lat_lon_hdbscan_cluster_10_03','bin','parent_category_name','item_id']; QQ=0;\n",
    "    if i==37: selcols=['lat_lon_hdbscan_cluster_20_03','bin','parent_category_name','item_id']; QQ=0;\n",
    "    if i==38: selcols=['lat_lon_hdbscan_cluster_05_03','bin_3','image_top_1']; QQ=4;\n",
    "    if i==39: selcols=['lat_lon_hdbscan_cluster_10_03','bin_3','image_top_1']; QQ=4;\n",
    "    if i==40: selcols=['lat_lon_hdbscan_cluster_20_03','bin_3','image_top_1']; QQ=4;\n",
    "    if i==41: selcols=['lat_lon_hdbscan_cluster_05_03','bin_3','image_top_1','item_id']; QQ=0;\n",
    "    if i==42: selcols=['lat_lon_hdbscan_cluster_10_03','bin_3','image_top_1','item_id']; QQ=0;\n",
    "    if i==43: selcols=['lat_lon_hdbscan_cluster_20_03','bin_3','image_top_1','item_id']; QQ=0;\n",
    "    if i==44: selcols=['region', 'city', 'category_name','user_id']; QQ=4;\n",
    "    if i==45: selcols=['price', 'param_3','user_id']; QQ=4;\n",
    "    if i==46: selcols=['price','param_1','param_2','param_3','user_id']; QQ=4;\n",
    "    if i==47: selcols=['price','category_name', 'parent_category_name','user_id']; QQ=4;\n",
    "    if i==48: selcols=['price', 'lat_lon_hdbscan_cluster_05_03','user_id']; QQ=4;\n",
    "    if i==49: selcols=['price', 'image_top_1','user_id']; QQ=4;\n",
    "    if i==50: selcols=['price', 'bin_3','user_id']; QQ=4;\n",
    "    if i==51: selcols=['price', 'bin_2','bin_3','category_name','user_id']; QQ=4;\n",
    "    if i==52: selcols=['image_top_1','bin_2','bin_3','user_id']; QQ=4;\n",
    "    if i==53: selcols=['image_top_1', 'city', 'region','user_id']; QQ=4;\n",
    "    if i==54: selcols=['image_top_1', 'category_name', 'parent_category_name','user_id']; QQ=4;\n",
    "    if i==55: selcols=['image_top_1', 'category_name', 'user_id']; QQ=4;\n",
    "    if i==56: selcols=['lat_lon_hdbscan_cluster_05_03','bin_2','item_id']; QQ=0;\n",
    "    if i==57: selcols=['lat_lon_hdbscan_cluster_10_03','bin_2','item_id']; QQ=0;\n",
    "    if i==58: selcols=['lat_lon_hdbscan_cluster_20_03','bin_2','item_id']; QQ=0;\n",
    "\n",
    "\n",
    "    print('selcols',selcols,'QQ',QQ)\n",
    "    filename = '_'.join(selcols) + '.csv'\n",
    "    if os.path.exists(filename):\n",
    "        print('here')\n",
    "        gp=pd.read_csv(filename)\n",
    "        train = train.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "    else:\n",
    "        if QQ==0:\n",
    "            gp = all_samples[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].count().reset_index().\\\n",
    "                rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "            if(gp.empty==False):\n",
    "                \n",
    "                train = train.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "                test = test.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            else:\n",
    "                empty.append('X'+str(i))\n",
    "        if QQ==1:\n",
    "            gp = all_samples[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].mean().reset_index().\\\n",
    "                rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "            train = train.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            test = test.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "        if QQ==2:\n",
    "            gp = all_samples[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].var().reset_index().\\\n",
    "                rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "            train = train.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            test = test.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "        if QQ==3:\n",
    "            gp = all_samples[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].skew().reset_index().\\\n",
    "                rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "            train = train.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            test = test.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "        if QQ==4:\n",
    "            gp = all_samples[selcols].groupby(by=selcols[0:len(selcols)-1])[selcols[len(selcols)-1]].nunique().reset_index().\\\n",
    "                rename(index=str, columns={selcols[len(selcols)-1]: 'X'+str(i)})\n",
    "            if(gp.empty==False):\n",
    "                   \n",
    "                train = train.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "                test = test.merge(gp, on=selcols[0:len(selcols)-1], how='left')\n",
    "            else:\n",
    "                empty.append('X'+str(i))\n",
    "      \n",
    "            \n",
    "        if (os.path.exists(filename)==False):\n",
    "            if(gp.empty==False):\n",
    "                gp.to_csv(filename,index=False)\n",
    "            \n",
    "    del gp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,naddfeat):\n",
    "    predictors.append('X'+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_seq_number'] = train['item_seq_number_temp'] \n",
    "test['item_seq_number'] = test['item_seq_number_temp'] \n",
    "#all_samples['item_seq_number'] = all_samples['item_seq_number_temp']\n",
    "train['price'] = train['price_temp']\n",
    "test['price'] = test['price_temp'] \n",
    "#all_samples['price'] = all_samples['price_temp'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = [ 'X7', 'X10', 'X11', 'X12','X2','X17','X44']\n",
    "for i in empty:\n",
    "    predictors.remove(i)\n",
    "del all_samples\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\n",
    "    'image_top_1', 'param_1', 'param_2', 'param_3', \n",
    "    'city', 'region', 'category_name', 'parent_category_name', 'user_type', 'reg_Time_zone','bin','bin_2','bin_3'\n",
    "]\n",
    "for feature in categorical:\n",
    "    print(f'Transforming {feature}...')\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train[feature].append(test[feature]).astype(str))\n",
    "    \n",
    "    train[feature] = encoder.transform(train[feature].astype(str))\n",
    "    test[feature] = encoder.transform(test[feature].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('intermediate.csv')\n",
    "test_features = pd.read_csv('test_intermediate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feat = ['nima_mean','nima_std','mobilenet_mean','mobilenet_std']\n",
    "for img_feat in image_feat:\n",
    "    train[img_feat] = features[img_feat]\n",
    "    test[img_feat] = test_features[img_feat]\n",
    "    predictors.append(img_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feat = ['imagenet_conf','imagenet_class']\n",
    "for img_feat in image_feat:\n",
    "    train[img_feat] = features[img_feat]\n",
    "    test[img_feat] = test_features[img_feat]\n",
    "    predictors.append(img_feat)\n",
    "categorical.append('imagenet_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feat = ['dullness','whiteness','average_pixel_width','average_red',\n",
    "'average_green','average_blue','image_size','blurrness', 'width','height']\n",
    "for img_feat in image_feat:\n",
    "    train[img_feat] = features[img_feat]\n",
    "    test[img_feat] = test_features[img_feat]\n",
    "    predictors.append(img_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_top_train = pd.read_csv(\"train_image_top_1_features.csv\") \n",
    "image_top_test = pd.read_csv(\"test_image_top_1_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['image_top_1'] = image_top_train['image_top_1']\n",
    "test['image_top_1'] = image_top_test['image_top_1']\n",
    "categoricals = [\n",
    "    'image_top_1'\n",
    "]\n",
    "for feature in categoricals:\n",
    "    print(f'Transforming {feature}...')\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(train[feature].append(test[feature]).astype(str))\n",
    "    \n",
    "    train[feature] = encoder.transform(train[feature].astype(str))\n",
    "    test[feature] = encoder.transform(test[feature].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TFIDF Vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,1))\n",
    "#ngram_range defines how you want to have words in your dictionary. (min,max) = (1,2) will mean you will have unigrams and bigrms in your vocabulary. \n",
    "#Example String: \"The old fox\"\n",
    "#Vocabulary: \"The\", \"old\", \"fox\", \"The old\", \"old fox\"\n",
    "\n",
    "full_tfidf = tfidf_vec.fit_transform(train['title'].values.tolist() + test['title'].values.tolist())\n",
    "#train_df['title'].values.tolist() this converts all the values in the title column into a list. '+' appends two lists\n",
    "\n",
    "train_tfidf = tfidf_vec.transform(train['title'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['title'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVD Components ###\n",
    "n_comp = 5\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "train_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\n",
    "train = pd.concat([train, train_svd], axis=1)\n",
    "test = pd.concat([test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n",
    "for i in range(n_comp):\n",
    "    predictors.append('svd_title_'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apps_as_matrix = CountTokenizer().fit_transform(apps_as_sentence)\n",
    "full_tfidf = tfidf_vec.fit_transform(train['title'].values.tolist() + test['title'].values.tolist())\n",
    "#train_df['title'].values.tolist() this converts all the values in the title column into a list. '+' appends two lists\n",
    "\n",
    "train_tfidf = tfidf_vec.transform(train['title'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['title'].values.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_comp=5\n",
    "lda_obj = LatentDirichletAllocation(n_components=n_comp)\n",
    "lda_obj.fit(full_tfidf)\n",
    "train_lda = pd.DataFrame(lda_obj.transform(train_tfidf))\n",
    "test_lda = pd.DataFrame(lda_obj.transform(test_tfidf))\n",
    "train_lda.columns = ['lda_title_'+str(i+1) for i in range(n_comp)]\n",
    "test_lda.columns = ['lda_title_'+str(i+1) for i in range(n_comp)]\n",
    "train = pd.concat([train, train_lda], axis=1)\n",
    "test = pd.concat([test, test_lda], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_lda, test_lda\n",
    "for i in range(n_comp):\n",
    "    predictors.append('lda_title_'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,1), max_features=100000)\n",
    "full_tfidf = tfidf_vec.fit_transform(train['description'].values.tolist() + test['description'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train['description'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['description'].values.tolist())\n",
    "\n",
    "### SVD Components ###\n",
    "n_comp = 5\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "train_svd.columns = ['svd_desc_'+str(i+1) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_desc_'+str(i+1) for i in range(n_comp)]\n",
    "train = pd.concat([train, train_svd], axis=1)\n",
    "test = pd.concat([test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n",
    "gc.collect()    \n",
    "for i in range(n_comp):\n",
    "    predictors.append('svd_desc_'+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder:\n",
    "    # Adapted from https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
    "    def __repr__(self):\n",
    "        return 'TargetEncoder'\n",
    "\n",
    "    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n",
    "        self.cols = cols\n",
    "        self.smoothing = smoothing\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.noise_level = noise_level\n",
    "        self.keep_original = keep_original\n",
    "\n",
    "    @staticmethod\n",
    "    def add_noise(series, noise_level):\n",
    "        return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "    def encode(self, train, test, target):\n",
    "        for col in self.cols:\n",
    "            if self.keep_original:\n",
    "                train[col + '_te'], test[col + '_te'] = self.encode_column(train[col], test[col], target)\n",
    "            else:\n",
    "                train[col], test[col] = self.encode_column(train[col], test[col], target)\n",
    "        return train, test\n",
    "\n",
    "    def encode_column(self, trn_series, tst_series, target):\n",
    "        temp = pd.concat([trn_series, target], axis=1)\n",
    "        # Compute target mean\n",
    "        averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "        # Compute smoothing\n",
    "        smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n",
    "        # Apply average function to all target data\n",
    "        prior = target.mean()\n",
    "        # The bigger the count the less full_avg is taken into account\n",
    "        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "        averages.drop(['mean', 'count'], axis=1, inplace=True)\n",
    "        # Apply averages to trn and tst series\n",
    "        ft_trn_series = pd.merge(\n",
    "            trn_series.to_frame(trn_series.name),\n",
    "            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "            on=trn_series.name,\n",
    "            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "        # pd.merge does not keep the index so restore it\n",
    "        ft_trn_series.index = trn_series.index\n",
    "        ft_tst_series = pd.merge(\n",
    "            tst_series.to_frame(tst_series.name),\n",
    "            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "            on=tst_series.name,\n",
    "            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "        # pd.merge does not keep the index so restore it\n",
    "        ft_tst_series.index = tst_series.index\n",
    "        return self.add_noise(ft_trn_series, self.noise_level), self.add_noise(ft_tst_series, self.noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cats = ['region','city','parent_category_name','category_name','user_type','image_top_1']\n",
    "target_encode = TargetEncoder(min_samples_leaf=100, smoothing=7, noise_level=0.01, keep_original=True, cols=f_cats)\n",
    "train, test = target_encode.encode(train, test, train['deal_probability'])\n",
    "for fc in f_cats:\n",
    "    predictors.append(fc+'_te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_feat = ['norm_price','bin','bin_2','price_bin_count','price_bin2_count','bin_3','price_bin3_count']\n",
    "for pf in price_feat:\n",
    "    predictors.append(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cats = ['bin','bin_2','bin_3']\n",
    "for fc in f_cats:\n",
    "    categorical.append(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cats = ['bin','bin_2','bin_3','param_2','param_1']\n",
    "target_encode = TargetEncoder(min_samples_leaf=100, smoothing=10, noise_level=0.01, keep_original=True, cols=f_cats)\n",
    "train, test = target_encode.encode(train, test, train['deal_probability'])\n",
    "for fc in f_cats:\n",
    "    predictors.append(fc+'_te')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"price\"].fillna(-999,inplace=True)\n",
    "train[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "#test[\"price\"] = np.log(test[\"price\"]+0.001)\n",
    "test[\"price\"].fillna(-999,inplace=True)\n",
    "test[\"image_top_1\"].fillna(-999,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 30000\n",
    "early_stop_rounds = 200\n",
    "\n",
    "feature_names = np.hstack([\n",
    "    count_vectorizer_desc.get_feature_names(),\n",
    "    count_vectorizer_title.get_feature_names(),\n",
    "    count_vectorizer_eng_desc.get_feature_names(),\n",
    "    count_vectorizer_eng_title.get_feature_names(),\n",
    "    predictors,\n",
    "    #tfvocab\n",
    "])\n",
    "print('Number of features:', len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scipy.sparse.hstack([\n",
    "    test_desc_counts,\n",
    "    test_title_counts,\n",
    "    test_eng_desc_counts,\n",
    "    test_eng_title_counts,\n",
    "    test[predictors],\n",
    "    #test_ready_df\n",
    "], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scipy.sparse.hstack([\n",
    "        train_desc_counts,\n",
    "        train_title_counts,\n",
    "        train_eng_desc_counts,\n",
    "        train_eng_title_counts,\n",
    "        train.loc[:,predictors],\n",
    "       # train_ready_df\n",
    "      #  train_ready_df.loc[train_index,:].values\n",
    "], format='csr')\n",
    "y_train = train[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta' :0.3,\n",
    "    'tree_method':\"hist\",\n",
    "    'grow_policy': \"lossguide\",\n",
    "    'max_leaves': 1400,\n",
    "    'max_depth' : 0 ,\n",
    "    'subsample' : 0.9,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bylevel':0.7,\n",
    "    'min_child_weight':0,\n",
    "    'alpha':4,\n",
    "    'objective': 'reg:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state':99,\n",
    "    'silent':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "RS = 1234921940\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=1020210)\n",
    "oof_preds = np.zeros(x_train.shape[0])\n",
    "\n",
    "test_predicts_list = []\n",
    "np.random.seed(RS)\n",
    "te_data = xgb.DMatrix(x_test)\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train)):\n",
    "    trn_x, trn_y = x_train[trn_idx], y_train[trn_idx].values\n",
    "    val_x, val_y = x_train[val_idx], y_train[val_idx].values\n",
    "    \n",
    "    tr_data = xgb.DMatrix(trn_x, label=trn_y)\n",
    "    va_data = xgb.DMatrix(val_x, label=val_y)\n",
    "    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n",
    "    model = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds=50, verbose_eval=5)\n",
    "    \n",
    "    preds = model.predict(te_data)\n",
    "    oof_preds[val_idx] = model.predict(val_x)\n",
    "    test_predicts_list.append(preds)\n",
    "    fig, ax = plt.subplots(figsize=(10, 14))\n",
    "    plot_importance(model)\n",
    "    plt.title(\"Light GBM Feature Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.read_csv('sample_submission.csv', usecols=['item_id'])\n",
    "subm['deal_probability'] = np.clip(final_preds, 0, 1)\n",
    "subm.to_csv('xgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xgb_cv_oof.npy',oof_preds)\n",
    "np.save('xgb_cv_preds.npy',final_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
